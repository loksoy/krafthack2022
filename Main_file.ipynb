{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd55f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "656e2bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in c:\\users\\person\\anaconda3\\lib\\site-packages (0.40.0)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\person\\anaconda3\\lib\\site-packages (from shap) (21.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\person\\anaconda3\\lib\\site-packages (from shap) (1.2.4)\n",
      "Requirement already satisfied: numba in c:\\users\\person\\anaconda3\\lib\\site-packages (from shap) (0.53.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\person\\anaconda3\\lib\\site-packages (from shap) (0.24.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\person\\anaconda3\\lib\\site-packages (from shap) (1.20.1)\n",
      "Requirement already satisfied: slicer==0.0.7 in c:\\users\\person\\anaconda3\\lib\\site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\person\\anaconda3\\lib\\site-packages (from shap) (1.6.0)\n",
      "Requirement already satisfied: tqdm>4.25.0 in c:\\users\\person\\anaconda3\\lib\\site-packages (from shap) (4.59.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\person\\anaconda3\\lib\\site-packages (from shap) (1.6.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\person\\anaconda3\\lib\\site-packages (from packaging>20.9->shap) (2.4.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\person\\anaconda3\\lib\\site-packages (from numba->shap) (52.0.0.post20210125)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in c:\\users\\person\\anaconda3\\lib\\site-packages (from numba->shap) (0.36.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\person\\anaconda3\\lib\\site-packages (from pandas->shap) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\person\\anaconda3\\lib\\site-packages (from pandas->shap) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\person\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\person\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\person\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "355ac0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import statsmodels as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import numpy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import r2_score as R2\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6228dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_true, y_pred):\n",
    "    return np.sqrt(MSE(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96d64f9",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797f0522",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ds1 = \"Krafthack2022/input_dataset-1.csv\"\n",
    "path_ds2 = \"Krafthack2022/input_dataset-2.csv\"\n",
    "path_pred = \"Krafthack2022/prediction.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9136b45b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds2 = pd.read_csv(path_ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09dfe9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = pd.read_csv(path_ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d9e75504",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pd.read_csv(path_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2a96d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [ds1, ds2]\n",
    "# full_df = pd.concat(frames).sample(100000)\n",
    "full_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8a65e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.drop(['lower_bearing_vib_vrt', 'turbine_bearing_vib_vrt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cb6956ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"timepoints\"] = pd.to_datetime(full_df[\"timepoints\"], format='%Y-%m-%d %H:%M:%S') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b3f790d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = full_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b5dc59fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df.index = copy_df.timepoints\n",
    "copy_df = copy_df.drop(\"timepoints\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "92a44dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = copy_df.sort_index()\n",
    "\n",
    "copy_df['seconds'] = copy_df.index - copy_df.index[0]\n",
    "\n",
    "copy_df['seconds'] = copy_df['seconds'].dt.total_seconds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd04c2",
   "metadata": {},
   "source": [
    "#### Dealing with NaN values\n",
    "In order to keep it simple we simply discarded rows of data with NaN values.\n",
    "\n",
    "An alternative approach would be to use for example K-nearest neighbors, with the sklearn\n",
    "KNNImputer, in this case you would ideally make a train/validation set for this specific purpose in order to choose n_neighbors. In order to avoid data leakage imputing should only be done on a training set,\n",
    "so it shouldn't solve NaN problems in the hold-out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6f2c84f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = copy_df.dropna()\n",
    "\n",
    "# imputer = KNNImputer(n_neighbors=2)\n",
    "# imputer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "322275d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = list(X_new.drop(\"timepoints\", axis=1).columns)\n",
    "x_cols.append(\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ab70b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y columns\n",
    "y_cols = [c for c in copy_df.columns if \"Tensile\" in c]\n",
    "Y_df = copy_df[y_cols]\n",
    "\n",
    "#X columns\n",
    "X_df = copy_df[x_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0cff9e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2c19ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,7))\n",
    "# figure=sns.heatmap(X_df.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75014a2",
   "metadata": {},
   "source": [
    "##### Removed feature\n",
    "We remove the following feature as it is 99% linearly correlated with another feature, so unless there is some very hidden non-linear correlation between the two, this should be fine.\n",
    "Specifically Turbine_Guide Vane Opening is highly correlated with Unit_4_Power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bd596078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping column: Unit_4_Power\n",
    "X_df = X_df.drop(\"Turbine_Guide Vane Opening\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709fadf2",
   "metadata": {},
   "source": [
    "##### Adding different time intervals\n",
    "\n",
    "In order to gauge whether there is some repeated (seasonal) patterns (e.g every monday the turbines are started) we add various time-period columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ffa3b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df[\"day\"] = X_df.index.day\n",
    "X_df[\"weekday\"] = X_df.index.weekday\n",
    "X_df[\"month\"] = X_df.index.month\n",
    "X_df[\"hour\"] = X_df.index.hour\n",
    "\n",
    "X_df_ = X_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8af5ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.get_dummies(X_df_, columns=[\"mode\"], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ecead",
   "metadata": {},
   "source": [
    "#### Splitting data and scaling the data\n",
    "We split the data in train, validation and test.\n",
    "We use the validation set for early stopping and hyperparameter tuning.\n",
    "The train and validation are shuffled. However, we keep the test set non-shuffled, not because the temporal order matter (this is not a time-series model), but because we want nice plots with no time-gaps (in a production scenario we wouldn't do it like this).\n",
    "\n",
    "We scale the data to the interval [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "948dd2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unit_4_Power', 'Unit_4_Reactive Power', 'Turbine_Pressure Drafttube', 'Turbine_Pressure Spiral Casing', 'Turbine_Rotational Speed', 'seconds', 'day', 'weekday', 'month', 'hour']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_df, Y_df, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1, shuffle=True) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "cols_scale = [c for c in X_train.columns if c != 'mode_start']\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train.loc[:, cols_scale])\n",
    "\n",
    "\n",
    "X_train.loc[:, cols_scale] = scaler.transform(X_train.loc[:, cols_scale])\n",
    "X_test.loc[:, cols_scale] = scaler.transform(X_test.loc[:, cols_scale])\n",
    "X_val.loc[:, cols_scale] = scaler.transform(X_val.loc[:, cols_scale])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c7e90a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_size = 10**5\n",
    "# X_train = X_train.sample(sample_size)\n",
    "# y_train = y_train.loc[X_train.index, :]\n",
    "\n",
    "# X_val = X_train.sample(sample_size)\n",
    "# y_val = y_train.loc[X_train.index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a331ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "963974af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Model():\n",
    "    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_val, self.y_val = X_val, y_val\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "        \n",
    "        self.model = None\n",
    "        self.pred_model = None\n",
    "        \n",
    "        self.features_num = self.X_train.shape[1]\n",
    "        self.targets_num = self.y_train.shape[1]\n",
    "        \n",
    "    def make_nn_model(self, layers=None,\n",
    "                      activation='relu', drop_rate=0.2, drop_rate_inference=False):\n",
    "        \n",
    "        if not drop_rate_inference:\n",
    "            self.layers, self.activation = layers, activation\n",
    "            dropout = Dropout\n",
    "        else:\n",
    "            layers, activation = self.layers, self.activation\n",
    "            #Using dropout during prediction for uncertainty\n",
    "            dropout = MCDropout\n",
    "            \n",
    "        inputs = Input(shape=(self.features_num,))\n",
    "        for i, lay in enumerate(layers):\n",
    "            if i == 0:\n",
    "                x = Dense(lay, activation='relu')(inputs)\n",
    "#                 x = BatchNormalization()(x)\n",
    "#                 x = dropout(drop_rate)(x)\n",
    "            else:\n",
    "                x = Dense(lay, activation='relu')(x)\n",
    "#                 x = BatchNormalization()(x)\n",
    "#                 x = dropout(drop_rate)(x)\n",
    "        outputs = Dense(self.targets_num, activation='linear')(x)\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"Evens_lille_venn\")\n",
    "        if not drop_rate_inference:\n",
    "#             model.summary()\n",
    "            self.model = model\n",
    "        else:\n",
    "            return model\n",
    "        \n",
    "    def for_tune(self, layers, activation, drop_rate, lr, epochs=4000, patience=10, batch_size=32, verbose=0):\n",
    "        tf.keras.backend.clear_session()\n",
    "        self.make_nn_model(layers, activation, drop_rate)\n",
    "        self.fitting(lr=lr, batch_size=batch_size, epochs=epochs, patience=patience, verbose=verbose)\n",
    "        \n",
    "        y_pred = self.model.predict(X_val)\n",
    "        \n",
    "        return y_pred\n",
    "        \n",
    "    \n",
    "    def fitting(self, epochs=1000, patience=100, batch_size=32, lr=0.00001, verbose=1, save_path=None):\n",
    "        if self.model is not None:\n",
    "            self.epochs, self.patience, self.batch_size, self.lr = epochs, patience, batch_size, lr\n",
    "            es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "            optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "            self.model.compile(optimizer, loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "            # Prepare the training dataset\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((self.X_train, self.y_train))\n",
    "            train_dataset = train_dataset.shuffle(buffer_size=4096).batch(batch_size)\n",
    "\n",
    "            # Prepare the validation dataset\n",
    "            val_dataset = tf.data.Dataset.from_tensor_slices((self.X_val, self.y_val))\n",
    "            val_dataset = val_dataset.batch(batch_size)\n",
    "            self.history = self.model.fit(train_dataset, validation_data=val_dataset, epochs=epochs, callbacks=[es_callback], verbose=verbose)\n",
    "            if save_path is not None:\n",
    "                self.model.save_weights(save_path)\n",
    "            return self.model, self.history\n",
    "        \n",
    "    def fit_on_all(self, epochs=1000, patience=100, batch_size=32, lr=0.00001, verbose=1, save_path=None):\n",
    "        if self.model is not None:\n",
    "            self.epochs, self.patience, self.batch_size, self.lr = epochs, patience, batch_size, lr\n",
    "            es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "            optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "            self.model.compile(optimizer, loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "            # Prepare the training dataset\n",
    "            X_train = pd.concat([self.X_train, self.X_test])\n",
    "            y_train = pd.concat([self.y_train, self.y_test])\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "            train_dataset = train_dataset.shuffle(buffer_size=4096).batch(batch_size)\n",
    "\n",
    "            # Prepare the validation dataset\n",
    "            val_dataset = tf.data.Dataset.from_tensor_slices((self.X_val, self.y_val))\n",
    "            val_dataset = val_dataset.batch(batch_size)\n",
    "            self.history = self.model.fit(train_dataset, validation_data=val_dataset, epochs=epochs, callbacks=[es_callback], verbose=verbose)\n",
    "            if save_path is not None:\n",
    "                self.model.save_weights(save_path)\n",
    "            return self.model, self.history        \n",
    "        \n",
    "    def load_model(self, path):\n",
    "        self.model.load_weights(path)\n",
    "#         for i, weights in enumerate(weights_list[0:9]):\n",
    "#             model.layers[i].set_weights(weights)\n",
    "    \n",
    "    def dropout_model(self, drop_rate_predict=0.2):\n",
    "        self.model_dropout = self.make_nn_model(drop_rate = drop_rate_predict, drop_rate_inference=True)\n",
    "        self.model_dropout.set_weights(self.model.get_weights())\n",
    "        return self.model_dropout\n",
    "        \n",
    "    def loss_plot(self):\n",
    "        hist = self.history.history\n",
    "        fig, ax = plt.subplots(figsize=(10, 15))\n",
    "        ax.plot(np.arange(len(hist['val_loss'])), hist['val_loss'], label='val_loss')\n",
    "        ax.plot(np.arange(len(hist['val_loss'])), hist['loss'], label='loss')\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "754cef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_predict(model, X, y, credible_interval=0.95, iterations=100):\n",
    "    if iterations < 2:\n",
    "        iterations = 2\n",
    "    predictions = np.zeros((y.shape[0], y.shape[1], iterations))\n",
    "    for i in range(iterations):\n",
    "        pred = model.predict(X)\n",
    "        predictions[:, :, i] = pred\n",
    "    y_pred = np.mean(predictions, axis=2)\n",
    "    lower = np.quantile(predictions, 0.5-credible_interval/2, axis=2)\n",
    "    upper = np.quantile(predictions, 0.5-credible_interval/2, axis=2)\n",
    "    \n",
    "    return y_pred, lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b97b17e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_grid_tuner(save_path='test'):\n",
    "    layers_ls = [[32, 32], [32, 32, 32], [32, 32, 32, 32], [64, 64, 64, 64], [128, 128, 128, 128]]\n",
    "    lr_ls = [0.001, 0.0005, 0.0001, 0.00001]\n",
    "    drop_rate_ls = [0.1, 0.2, 0.3, 0.4]\n",
    "    activation_ls = ['relu', 'swish']\n",
    "    batch_size_ls = [4096]\n",
    "\n",
    "    grid = {'layers':layers_ls, 'lr':lr_ls, 'drop_rate':drop_rate_ls, 'activation':activation_ls, 'batch_size': batch_size_ls}\n",
    "\n",
    "    ix_ls = []\n",
    "\n",
    "    mape_best = 100\n",
    "    rounds = 5\n",
    "    np.random.seed(43)\n",
    "    for i in range(rounds):\n",
    "        print(\"\")\n",
    "        print(\"Round: \", i)\n",
    "        round = 0\n",
    "        while True:\n",
    "            ixs = []\n",
    "            use_vals = {}\n",
    "            for key, val in grid.items():\n",
    "                ix = np.random.randint(0, high=len(val))\n",
    "                ixs.append(ix)\n",
    "                use_vals[key] = val[ix]\n",
    "            again = False\n",
    "            for ix_in in ix_ls:\n",
    "                if ixs == ix_in:\n",
    "                    again = True\n",
    "            if not again:\n",
    "                break\n",
    "            else:\n",
    "                round += 1\n",
    "            if round == 3:\n",
    "                break\n",
    "        print(\"Using params: \", use_vals)\n",
    "        y_val_pred = nn.for_tune(**use_vals, verbose=2)\n",
    "        mape = MAPE(y_val, y_val_pred)\n",
    "        r2 = R2(y_val, y_val_pred)\n",
    "        print(\"MAPE: \", mape)\n",
    "        print(\"R2: \", r2)\n",
    "        if mape < mape_best:\n",
    "            mape_best = mape\n",
    "            best_model = nn.model\n",
    "            best_params = use_vals\n",
    "            print(\"\")\n",
    "            print(\"***New Best***\")\n",
    "            print(\"Best params: \", best_params)\n",
    "            print(\"Best MAPE: \", mape_best)\n",
    "            print(\"Best r2: \", r2)\n",
    "\n",
    "    print(\"mape_best\", mape_best)\n",
    "    print(\"best_params\", best_params)\n",
    "    best_model.save_weights(save_path)\n",
    "    return best_model, mape_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "870cd273",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NN_Model(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "nn.make_nn_model(layers=[20, 20, 20], drop_rate=0.05, activation='relu')\n",
    "# nn.make_nn_model(layers=[20, 20, 20], drop_rate=0.3, activation='relu')\n",
    "\n",
    "# best_model, mape_best = random_grid_tuner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f542dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.model.save_weights('temp_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "238c1364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "64735/64735 [==============================] - 42s 648us/step - loss: 257.1287 - root_mean_squared_error: 16.0351 - val_loss: 2765.3699 - val_root_mean_squared_error: 52.5868\n",
      "Epoch 2/50\n",
      "64735/64735 [==============================] - 43s 664us/step - loss: 83.8858 - root_mean_squared_error: 9.1590 - val_loss: 2024.0580 - val_root_mean_squared_error: 44.9896\n",
      "Epoch 3/50\n",
      "64735/64735 [==============================] - 43s 671us/step - loss: 66.1663 - root_mean_squared_error: 8.1344 - val_loss: 1669.6399 - val_root_mean_squared_error: 40.8613\n",
      "Epoch 4/50\n",
      "64735/64735 [==============================] - 120s 2ms/step - loss: 54.7521 - root_mean_squared_error: 7.3994 - val_loss: 1668.0266 - val_root_mean_squared_error: 40.8415\n",
      "Epoch 5/50\n",
      "64735/64735 [==============================] - 54s 841us/step - loss: 51.4238 - root_mean_squared_error: 7.1709 - val_loss: 1718.3999 - val_root_mean_squared_error: 41.4536\n",
      "Epoch 6/50\n",
      "64735/64735 [==============================] - 43s 659us/step - loss: 48.8819 - root_mean_squared_error: 6.9915 - val_loss: 1716.6464 - val_root_mean_squared_error: 41.4325\n",
      "Epoch 7/50\n",
      "64735/64735 [==============================] - 43s 667us/step - loss: 46.0624 - root_mean_squared_error: 6.7868 - val_loss: 1598.8148 - val_root_mean_squared_error: 39.9852\n",
      "Epoch 8/50\n",
      "64735/64735 [==============================] - 43s 664us/step - loss: 42.7893 - root_mean_squared_error: 6.5414 - val_loss: 1536.8859 - val_root_mean_squared_error: 39.2031\n",
      "Epoch 9/50\n",
      "64735/64735 [==============================] - 43s 666us/step - loss: 39.3266 - root_mean_squared_error: 6.2712 - val_loss: 1489.0710 - val_root_mean_squared_error: 38.5885\n",
      "Epoch 10/50\n",
      "64735/64735 [==============================] - 43s 666us/step - loss: 35.6049 - root_mean_squared_error: 5.9671 - val_loss: 1435.8546 - val_root_mean_squared_error: 37.8927\n",
      "Epoch 11/50\n",
      "64735/64735 [==============================] - 43s 668us/step - loss: 31.6013 - root_mean_squared_error: 5.6214 - val_loss: 1375.2603 - val_root_mean_squared_error: 37.0845\n",
      "Epoch 12/50\n",
      "64735/64735 [==============================] - 43s 660us/step - loss: 27.3274 - root_mean_squared_error: 5.2275 - val_loss: 1321.5830 - val_root_mean_squared_error: 36.3536\n",
      "Epoch 13/50\n",
      "64735/64735 [==============================] - 49s 760us/step - loss: 23.0443 - root_mean_squared_error: 4.8004 - val_loss: 1271.7649 - val_root_mean_squared_error: 35.6619\n",
      "Epoch 14/50\n",
      "64735/64735 [==============================] - 50s 778us/step - loss: 19.1627 - root_mean_squared_error: 4.3776 - val_loss: 1708.9324 - val_root_mean_squared_error: 41.3392\n",
      "Epoch 15/50\n",
      "64735/64735 [==============================] - 42s 656us/step - loss: 17.0531 - root_mean_squared_error: 4.1296 - val_loss: 1653.4384 - val_root_mean_squared_error: 40.6625\n",
      "Epoch 16/50\n",
      "64735/64735 [==============================] - 43s 658us/step - loss: 14.4539 - root_mean_squared_error: 3.8018 - val_loss: 1604.8190 - val_root_mean_squared_error: 40.0601\n",
      "Epoch 17/50\n",
      "64735/64735 [==============================] - 42s 651us/step - loss: 12.3986 - root_mean_squared_error: 3.5211 - val_loss: 1550.2683 - val_root_mean_squared_error: 39.3734\n",
      "Epoch 18/50\n",
      "64735/64735 [==============================] - 44s 685us/step - loss: 10.8072 - root_mean_squared_error: 3.2875 - val_loss: 1488.5109 - val_root_mean_squared_error: 38.5812\n",
      "Epoch 19/50\n",
      "64735/64735 [==============================] - 43s 668us/step - loss: 9.5891 - root_mean_squared_error: 3.0967 - val_loss: 1428.1619 - val_root_mean_squared_error: 37.7909\n",
      "Epoch 20/50\n",
      "64735/64735 [==============================] - 43s 657us/step - loss: 8.7262 - root_mean_squared_error: 2.9540 - val_loss: 1366.7120 - val_root_mean_squared_error: 36.9690\n",
      "Epoch 21/50\n",
      "64735/64735 [==============================] - 43s 663us/step - loss: 8.1242 - root_mean_squared_error: 2.8503 - val_loss: 1309.9362 - val_root_mean_squared_error: 36.1930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.functional.Functional at 0x1d84e104dc0>,\n",
       " <tensorflow.python.keras.callbacks.History at 0x1d9187b52e0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.load_model('temp_model_2')\n",
    "nn.load_model('good_model')\n",
    "\n",
    "# \n",
    "nn.fit_on_all(lr=0.00001, batch_size=128, epochs=50, patience=8)\n",
    "nn.model.save_weights('final_model')\n",
    "# nn.fitting(lr=0.00001, batch_size=32, epochs=200, patience=5, save_path='temp_model_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "68670646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.model.save_weights('final_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0c8eff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12138/12138 [==============================] - 42s 3ms/step - loss: 46.4968 - root_mean_squared_error: 6.8189 - val_loss: 7.1538 - val_root_mean_squared_error: 2.6746\n",
      "Epoch 2/50\n",
      "12138/12138 [==============================] - 41s 3ms/step - loss: 7.1248 - root_mean_squared_error: 2.6692 - val_loss: 7.0863 - val_root_mean_squared_error: 2.6620\n",
      "Epoch 3/50\n",
      "12138/12138 [==============================] - 42s 3ms/step - loss: 7.0748 - root_mean_squared_error: 2.6598 - val_loss: 7.0462 - val_root_mean_squared_error: 2.6545\n",
      "Epoch 4/50\n",
      "12138/12138 [==============================] - 38s 3ms/step - loss: 7.0440 - root_mean_squared_error: 2.6540 - val_loss: 7.0228 - val_root_mean_squared_error: 2.6501\n",
      "Epoch 5/50\n",
      "12138/12138 [==============================] - 42s 3ms/step - loss: 7.0252 - root_mean_squared_error: 2.6505 - val_loss: 7.0068 - val_root_mean_squared_error: 2.6470\n",
      "Epoch 6/50\n",
      "12138/12138 [==============================] - 41s 3ms/step - loss: 7.0129 - root_mean_squared_error: 2.6482 - val_loss: 7.0009 - val_root_mean_squared_error: 2.6459\n",
      "Epoch 7/50\n",
      "12138/12138 [==============================] - 31s 3ms/step - loss: 7.0044 - root_mean_squared_error: 2.6466 - val_loss: 6.9928 - val_root_mean_squared_error: 2.6444\n",
      "Epoch 8/50\n",
      "12138/12138 [==============================] - 32s 3ms/step - loss: 6.9978 - root_mean_squared_error: 2.6453 - val_loss: 6.9835 - val_root_mean_squared_error: 2.6426\n",
      "Epoch 9/50\n",
      "12138/12138 [==============================] - 32s 3ms/step - loss: 6.9922 - root_mean_squared_error: 2.6443 - val_loss: 6.9810 - val_root_mean_squared_error: 2.6422\n",
      "Epoch 10/50\n",
      "12138/12138 [==============================] - 31s 3ms/step - loss: 6.9872 - root_mean_squared_error: 2.6433 - val_loss: 6.9752 - val_root_mean_squared_error: 2.6411\n",
      "Epoch 11/50\n",
      "12138/12138 [==============================] - 32s 3ms/step - loss: 6.9826 - root_mean_squared_error: 2.6425 - val_loss: 6.9698 - val_root_mean_squared_error: 2.6400\n",
      "Epoch 12/50\n",
      "12138/12138 [==============================] - 32s 3ms/step - loss: 6.9785 - root_mean_squared_error: 2.6417 - val_loss: 6.9709 - val_root_mean_squared_error: 2.6403\n",
      "Epoch 13/50\n",
      "12138/12138 [==============================] - 32s 3ms/step - loss: 6.9742 - root_mean_squared_error: 2.6409 - val_loss: 6.9625 - val_root_mean_squared_error: 2.6387\n",
      "Epoch 14/50\n",
      "12138/12138 [==============================] - 38s 3ms/step - loss: 6.9703 - root_mean_squared_error: 2.6401 - val_loss: 6.9683 - val_root_mean_squared_error: 2.6398\n",
      "Epoch 15/50\n",
      "12138/12138 [==============================] - 31s 3ms/step - loss: 6.9663 - root_mean_squared_error: 2.6394 - val_loss: 6.9593 - val_root_mean_squared_error: 2.6381\n",
      "Epoch 16/50\n",
      "12138/12138 [==============================] - 31s 3ms/step - loss: 6.9623 - root_mean_squared_error: 2.6386 - val_loss: 6.9516 - val_root_mean_squared_error: 2.6366\n",
      "Epoch 17/50\n",
      "12138/12138 [==============================] - 37s 3ms/step - loss: 6.9583 - root_mean_squared_error: 2.6379 - val_loss: 6.9464 - val_root_mean_squared_error: 2.6356\n",
      "Epoch 18/50\n",
      "12138/12138 [==============================] - 32s 3ms/step - loss: 6.9546 - root_mean_squared_error: 2.6372 - val_loss: 6.9419 - val_root_mean_squared_error: 2.6347\n",
      "Epoch 19/50\n",
      "12138/12138 [==============================] - 33s 3ms/step - loss: 6.9509 - root_mean_squared_error: 2.6364 - val_loss: 6.9380 - val_root_mean_squared_error: 2.6340\n",
      "Epoch 20/50\n",
      "12138/12138 [==============================] - 33s 3ms/step - loss: 6.9470 - root_mean_squared_error: 2.6357 - val_loss: 6.9362 - val_root_mean_squared_error: 2.6337\n",
      "Epoch 21/50\n",
      "12138/12138 [==============================] - 42s 3ms/step - loss: 6.9430 - root_mean_squared_error: 2.6350 - val_loss: 6.9315 - val_root_mean_squared_error: 2.6328\n",
      "Epoch 22/50\n",
      "12138/12138 [==============================] - 43s 4ms/step - loss: 6.9394 - root_mean_squared_error: 2.6343 - val_loss: 6.9275 - val_root_mean_squared_error: 2.6320\n",
      "Epoch 23/50\n",
      "12138/12138 [==============================] - 43s 4ms/step - loss: 6.9355 - root_mean_squared_error: 2.6335 - val_loss: 6.9238 - val_root_mean_squared_error: 2.6313\n",
      "Epoch 24/50\n",
      "12138/12138 [==============================] - 44s 4ms/step - loss: 6.9316 - root_mean_squared_error: 2.6328 - val_loss: 6.9193 - val_root_mean_squared_error: 2.6304\n",
      "Epoch 25/50\n",
      "12138/12138 [==============================] - 44s 4ms/step - loss: 6.9276 - root_mean_squared_error: 2.6320 - val_loss: 6.9218 - val_root_mean_squared_error: 2.6309\n",
      "Epoch 26/50\n",
      "12138/12138 [==============================] - 44s 4ms/step - loss: 6.9238 - root_mean_squared_error: 2.6313 - val_loss: 6.9112 - val_root_mean_squared_error: 2.6289\n",
      "Epoch 27/50\n",
      "12138/12138 [==============================] - 44s 4ms/step - loss: 6.9197 - root_mean_squared_error: 2.6305 - val_loss: 6.9081 - val_root_mean_squared_error: 2.6283\n",
      "Epoch 28/50\n",
      "12138/12138 [==============================] - 43s 4ms/step - loss: 6.9157 - root_mean_squared_error: 2.6298 - val_loss: 6.9151 - val_root_mean_squared_error: 2.6297\n",
      "Epoch 29/50\n",
      "12138/12138 [==============================] - 44s 4ms/step - loss: 6.9115 - root_mean_squared_error: 2.6290 - val_loss: 6.9013 - val_root_mean_squared_error: 2.6270\n",
      "Epoch 30/50\n",
      "12138/12138 [==============================] - 43s 4ms/step - loss: 6.9072 - root_mean_squared_error: 2.6282 - val_loss: 6.9004 - val_root_mean_squared_error: 2.6269\n",
      "Epoch 31/50\n",
      "12138/12138 [==============================] - 43s 4ms/step - loss: 6.9027 - root_mean_squared_error: 2.6273 - val_loss: 6.8902 - val_root_mean_squared_error: 2.6249\n",
      "Epoch 32/50\n",
      "12138/12138 [==============================] - 44s 4ms/step - loss: 6.8976 - root_mean_squared_error: 2.6263 - val_loss: 6.8879 - val_root_mean_squared_error: 2.6245\n",
      "Epoch 33/50\n",
      "12138/12138 [==============================] - 43s 4ms/step - loss: 6.8929 - root_mean_squared_error: 2.6254 - val_loss: 6.8806 - val_root_mean_squared_error: 2.6231\n",
      "Epoch 34/50\n",
      "12138/12138 [==============================] - 44s 4ms/step - loss: 6.8882 - root_mean_squared_error: 2.6245 - val_loss: 6.8905 - val_root_mean_squared_error: 2.6250\n",
      "Epoch 35/50\n",
      "12138/12138 [==============================] - 43s 4ms/step - loss: 6.8835 - root_mean_squared_error: 2.6236 - val_loss: 6.8728 - val_root_mean_squared_error: 2.6216\n",
      "Epoch 36/50\n",
      "12138/12138 [==============================] - 44s 4ms/step - loss: 6.8789 - root_mean_squared_error: 2.6228 - val_loss: 6.8710 - val_root_mean_squared_error: 2.6213\n",
      "Epoch 37/50\n",
      "12138/12138 [==============================] - 47s 4ms/step - loss: 6.8743 - root_mean_squared_error: 2.6219 - val_loss: 6.8628 - val_root_mean_squared_error: 2.6197\n",
      "Epoch 38/50\n",
      "12138/12138 [==============================] - 33s 3ms/step - loss: 6.8690 - root_mean_squared_error: 2.6209 - val_loss: 6.8573 - val_root_mean_squared_error: 2.6187\n",
      "Epoch 39/50\n",
      "12138/12138 [==============================] - 34s 3ms/step - loss: 6.8619 - root_mean_squared_error: 2.6195 - val_loss: 6.8510 - val_root_mean_squared_error: 2.6174\n",
      "Epoch 40/50\n",
      "12138/12138 [==============================] - 32s 3ms/step - loss: 6.8555 - root_mean_squared_error: 2.6183 - val_loss: 6.8541 - val_root_mean_squared_error: 2.6180\n",
      "Epoch 41/50\n",
      "12138/12138 [==============================] - 31s 3ms/step - loss: 6.8489 - root_mean_squared_error: 2.6170 - val_loss: 6.8408 - val_root_mean_squared_error: 2.6155\n",
      "Epoch 42/50\n",
      "12138/12138 [==============================] - 31s 3ms/step - loss: 6.8427 - root_mean_squared_error: 2.6159 - val_loss: 6.8359 - val_root_mean_squared_error: 2.6146\n",
      "Epoch 43/50\n",
      "12138/12138 [==============================] - 30s 2ms/step - loss: 6.8364 - root_mean_squared_error: 2.6147 - val_loss: 6.8265 - val_root_mean_squared_error: 2.6127\n",
      "Epoch 44/50\n",
      "12138/12138 [==============================] - 31s 3ms/step - loss: 6.8287 - root_mean_squared_error: 2.6132 - val_loss: 6.8160 - val_root_mean_squared_error: 2.6108\n",
      "Epoch 45/50\n",
      "12138/12138 [==============================] - 31s 3ms/step - loss: 6.8113 - root_mean_squared_error: 2.6099 - val_loss: 6.7908 - val_root_mean_squared_error: 2.6059\n",
      "Epoch 46/50\n",
      "12138/12138 [==============================] - 30s 2ms/step - loss: 6.7889 - root_mean_squared_error: 2.6056 - val_loss: 6.7744 - val_root_mean_squared_error: 2.6028\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12138/12138 [==============================] - 31s 3ms/step - loss: 6.7750 - root_mean_squared_error: 2.6029 - val_loss: 6.7623 - val_root_mean_squared_error: 2.6004\n",
      "Epoch 48/50\n",
      "12138/12138 [==============================] - 31s 3ms/step - loss: 6.7583 - root_mean_squared_error: 2.5997 - val_loss: 6.7450 - val_root_mean_squared_error: 2.5971\n",
      "Epoch 49/50\n",
      "12138/12138 [==============================] - 33s 3ms/step - loss: 6.7356 - root_mean_squared_error: 2.5953 - val_loss: 6.7262 - val_root_mean_squared_error: 2.5935\n",
      "Epoch 50/50\n",
      "12138/12138 [==============================] - 31s 3ms/step - loss: 6.7054 - root_mean_squared_error: 2.5895 - val_loss: 6.6838 - val_root_mean_squared_error: 2.5853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.functional.Functional at 0x1d84e104dc0>,\n",
       " <tensorflow.python.keras.callbacks.History at 0x1d843479880>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.load_model('final_model')\n",
    "nn.fitting(lr=0.00001, batch_size=512, epochs=50, patience=20, save_path='final_model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7ea8c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.load_model('final_model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9bdb203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pd.get_dummies(X_new, columns=[\"mode\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "52c402d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new[\"timepoints\"] = pd.to_datetime(X_new[\"timepoints\"], format='%Y-%m-%d %H:%M:%S') \n",
    "X_new.index = X_new.timepoints\n",
    "X_new = X_new.drop(\"timepoints\", axis=1)\n",
    "\n",
    "X_new = X_new.sort_index()\n",
    "\n",
    "X_new['seconds'] = X_new.index - copy_df.index[0]\n",
    "\n",
    "X_new['seconds'] = X_new['seconds'].dt.total_seconds()\n",
    "\n",
    "X_new[\"day\"] = X_new.index.day\n",
    "X_new[\"weekday\"] = X_new.index.weekday\n",
    "X_new[\"month\"] = X_new.index.month\n",
    "X_new[\"hour\"] = X_new.index.hour\n",
    "\n",
    "cols_scale = [c for c in X_new.columns if c != 'mode_start']\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_new.loc[:, cols_scale])\n",
    "\n",
    "\n",
    "X_new.loc[:, cols_scale] = scaler.transform(X_new.loc[:, cols_scale])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9391fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_new.drop('Turbine_Guide Vane Opening', axis=1)\n",
    "X_new = X_new[X_test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "eb64c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new = nn.model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c45be6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new_df = pd.DataFrame(y_pred, columns=y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "08365993",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new_df.index = X_new.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d3c32b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new_df.to_csv('solution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443881c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15, 15))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# y_test_sort = y_test.sort_index()\n",
    "# X_test_sort = X_test.sort_index()\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    rmse = RMSE(y_test.iloc[:, i], y_pred[:, i])\n",
    "    r2 = R2(y_test.iloc[:, i], y_pred[:, i])\n",
    "    mape = MAPE(y_test.iloc[:, i], y_pred[:, i])\n",
    "    ax.scatter(y_test.iloc[:, i], y_pred[:, i], label='r2: %s, mape %s' %(np.round(r2, 5), np.round(mape, 5)))\n",
    "    ax.set_xlabel(r'y_test ($\\mu m / m $)', fontsize=10)\n",
    "    ax.set_ylabel(r'y_pred ($\\mu m / m $)', fontsize=10)\n",
    "    ax.set_title('Tensil %s' %(i))\n",
    "    ax.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1409a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# model, history = fitting(model, X_train, y_train, X_val, y_val, epochs=1000, patience=10, batch_size=32, lr=0.00001)\n",
    "\n",
    "# hist = history.history\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
